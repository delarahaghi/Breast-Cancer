# -*- coding: utf-8 -*-
"""Breast_Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WPrH2SJUnSs9hc8as_LDAXak2lhkYlay

**Import Library**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.neighbors import KNeighborsClassifier
# %matplotlib inline

"""**Understanding the data**"""

df = pd.read_csv('/content/drive/MyDrive/BreastCancer.csv')

df.head()

df.shape

df.info()

df.columns

df.isnull().sum()

df.drop("Unnamed: 32", axis=1, inplace=True)

df.drop('id',axis=1, inplace=True)

df.describe()

"""**Data Visualizations**"""

plt.figure(figsize = (8,7))
sns.countplot(x="diagnosis", data=df, palette='magma')

#heatmap
plt.figure(figsize=(20,18))
sns.heatmap(df.corr(), annot=True,linewidths=.5, cmap="Purples")

df.columns

m_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

# Getting Se Columns with diagnosis
s_col = ['diagnosis','radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se']

# Getting Worst column with diagnosis
w_col = ['diagnosis','radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']

"""**Mean Columns**"""

sns.pairplot(df[m_col],hue = 'diagnosis', palette='Blues')

"""**For SE columns**"""

sns.pairplot(df[s_col],hue = 'diagnosis', palette='Greens')

"""**For Worst columns**"""

sns.pairplot(df[w_col],hue = 'diagnosis', palette='Oranges')

"""**Data Preprocessing and building model**"""

#Preprocessing
# counts of unique rows in the 'diagnosis' column
df['diagnosis'].value_counts()

# mapping categorical values to numerical values
df['diagnosis']=df['diagnosis'].map({'B':0,'M':1})

df['diagnosis'].value_counts()

"""**Splitting the data into train and test**"""

X_train, X_test, y_train, y_test = train_test_split(
                df.drop('diagnosis', axis=1),
                df['diagnosis'],
                test_size=0.2,
                random_state=42)

print("Shape of training set:", X_train.shape)
print("Shape of test set:", X_test.shape)

ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.fit_transform(X_test)

"""**Model**

1. Logistic Regression
"""

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
predictions1 = logreg.predict(X_test)

print("Confusion Matrix: \n", confusion_matrix(y_test, predictions1))
print('\n')
print(classification_report(y_test, predictions1))

logreg_acc = accuracy_score(y_test, predictions1)
print("Accuracy of the Logistic Regression Model is: %", logreg_acc*100)

"""2. KNN(K-Nearest Neighbors)"""

# to find which value shows the lowest mean error
error_rate = []

for i in range(1,42):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(12,6))
plt.plot(range(1,42), error_rate, color='purple', linestyle="--",
         marker='o', markersize=10, markerfacecolor='b')
plt.title('Error_Rate vs K-value')
plt.show()

knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(X_train, y_train)
predictions2 = knn.predict(X_test)

print(confusion_matrix(y_test, predictions2))
print("\n")
print(classification_report(y_test, predictions2))

knn_model_acc = accuracy_score(y_test, predictions2)
print("Accuracy of K Neighbors Classifier Model is: %", knn_model_acc*100)

"""3. Random Forests"""

rfc = RandomForestClassifier(n_estimators=300)
rfc.fit(X_train, y_train)
predictions4 = rfc.predict(X_test)

print("Confusion Matrix: \n", confusion_matrix(y_test, predictions4))
print("\n")
print(classification_report(y_test, predictions4))

rfc_acc = accuracy_score(y_test, predictions4)
print("Accuracy of Random Forests Model is: %", rfc_acc*100)

"""4. SVM(Support Vector Machines)"""

svc_model = SVC(kernel="rbf")
svc_model.fit(X_train, y_train)
predictions5 = svc_model.predict(X_test)

print("Confusion Matrix: \n", confusion_matrix(y_test, predictions5))
print("\n")
print(classification_report(y_test, predictions5))

svm_acc = accuracy_score(y_test, predictions5)
print("Accuracy of SVM model is: %", svm_acc*100)

"""5. Decision Tree Classifier"""

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
DecisionTreeAccuracy = accuracy_score(y_test, y_pred)
print("Accuracy of Decision Tree model is: %", DecisionTreeAccuracy*100)

"""6. Naive Bayes"""

clf = GaussianNB()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
NaiveBayesAccuracy = accuracy_score(y_test, y_pred)
print("Accuracy of Naive Bayes model is: %", NaiveBayesAccuracy*100)

"""7. ANN(Artificial Neural Network)"""

model = Sequential()
model.add(Dense(units=32, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(units=16, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
y_pred = (model.predict(X_test) > 0.5).astype(int)
ANNAccuracy = accuracy_score(y_test, y_pred)
print("Accuracy of ANN model is: %", ANNAccuracy*100)

"""Gradient Boosting"""

clf = xgb.XGBClassifier(objective='binary:logistic', random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
GradientBoostingAccuracy = accuracy_score(y_test, y_pred)
print("Accuracy of ANN model is: %", GradientBoostingAccuracy*100)

"""**Final Results**"""

print("The accuracy of Logistic Regression Model is : %",logreg_acc*100)
print("The accuracy of KNN Model is : %" ,knn_model_acc*100)
print("The accuracy of Random Forests Model is : %",rfc_acc*100)
print("The accuracy of SVM Model is : %",svm_acc*100)
print("The accuracy of Decision Tree Classifier Model is : %",DecisionTreeAccuracy*100)
print("The accuracy of Naive Bayes Model is : %" ,NaiveBayesAccuracy*100)
print("The accuracy of ANN Model is : %",ANNAccuracy*100)